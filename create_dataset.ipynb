{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to generate Packet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscapy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mscapy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscapy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IP, TCP, UDP\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deque\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import scapy.all as scapy\n",
    "from scapy.layers.inet import IP, TCP, UDP\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import subprocess\n",
    "import datetime\n",
    "\n",
    "# Constants\n",
    "SEQUENCE_LENGTH = 5\n",
    "NUM_FEATURES = 10\n",
    "CAPTURE_DURATION = None  # Capture for 3 hrs\n",
    "def extract_features(packet):\n",
    "    features = [0] * NUM_FEATURES\n",
    "\n",
    "    if IP in packet:\n",
    "        features[0] = len(packet)\n",
    "        features[1] = packet[IP].tos\n",
    "        features[2] = packet[IP].ttl\n",
    "        features[3] = packet[IP].proto\n",
    "        \n",
    "        if TCP in packet:\n",
    "            features[4] = packet[TCP].sport\n",
    "            features[5] = packet[TCP].dport\n",
    "            features[6] = packet[TCP].window\n",
    "            features[7] = len(packet[TCP].payload)\n",
    "        elif UDP in packet:\n",
    "            features[4] = packet[UDP].sport\n",
    "            features[5] = packet[UDP].dport\n",
    "            features[7] = len(packet[UDP].payload)\n",
    "\n",
    "        src_ip = packet[IP].src\n",
    "        dst_ip = packet[IP].dst\n",
    "        features[8] = int(src_ip.split('.')[-1])  # Last octet of source IP\n",
    "        features[9] = int(dst_ip.split('.')[-1])  # Last octet of destination IP\n",
    "\n",
    "    return features\n",
    "\n",
    "def classify_packet(packet):\n",
    "    if IP in packet:\n",
    "        if TCP in packet:\n",
    "            dport = packet[TCP].dport\n",
    "            sport = packet[TCP].sport\n",
    "        elif UDP in packet:\n",
    "            dport = packet[UDP].dport\n",
    "            sport = packet[UDP].sport\n",
    "        else:\n",
    "            return 'Normal'  # Neither TCP nor UDP\n",
    "\n",
    "        if dport == 53 or sport == 53:  # DNS\n",
    "            return 'Real Time'\n",
    "        elif dport in [80, 443]:  # HTTP/HTTPS\n",
    "            return 'Web download'\n",
    "        elif dport in [3074, 3075]:  # Xbox Live\n",
    "            return 'Games'\n",
    "        elif dport in [1935, 1936, 5222]:  # RTMP, XMPP\n",
    "            return 'Streaming'\n",
    "        elif dport == 22:  # SSH\n",
    "            return 'Real Time'\n",
    "        elif dport in [123]:  # NTP\n",
    "            return 'Real Time'\n",
    "    \n",
    "    return 'Normal'  # Default classification\n",
    "def get_wifi_interface():\n",
    "    try:\n",
    "        # Run netsh command to get WiFi interface information\n",
    "        result = subprocess.run([\"netsh\", \"wlan\", \"show\", \"interfaces\"], capture_output=True, text=True)\n",
    "        output = result.stdout\n",
    "\n",
    "        # Parse the output to find the name of the connected WiFi interface\n",
    "        for line in output.split('\\n'):\n",
    "            if \"Name\" in line:\n",
    "                return line.split(':')[1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting WiFi interface: {e}\")\n",
    "    return None\n",
    "\n",
    "def capture_packets(interface):\n",
    "    packet_buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    features_list = []\n",
    "    labels = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    def packet_callback(packet):\n",
    "        nonlocal features_list, labels\n",
    "\n",
    "        features = extract_features(packet)\n",
    "        packet_buffer.append(features)\n",
    "\n",
    "        if len(packet_buffer) == SEQUENCE_LENGTH:\n",
    "            features_list.append(list(packet_buffer))\n",
    "            labels.append(classify_packet(packet))\n",
    "\n",
    "        if time.time() - start_time > CAPTURE_DURATION:\n",
    "            return True  # Stop capture\n",
    "\n",
    "    print(f\"Capturing packets on {interface} for {CAPTURE_DURATION} seconds...\")\n",
    "    scapy.sniff(iface=interface, prn=packet_callback, store=False, stop_filter=lambda p: packet_callback(p))\n",
    "\n",
    "    return np.array(features_list), labels\n",
    "\n",
    "def main():\n",
    "\n",
    "    global CAPTURE_DURATION\n",
    "    print(\"5 min = 300 sec\\n10min = 600 sec\\n1hr = 3600sec\\n\")\n",
    "    CAPTURE_DURATION = int(input(\"Enter Capture Duration in seconds:\"))\n",
    "\n",
    "    interface = get_wifi_interface()\n",
    "    if not interface:\n",
    "        print(\"Could not find WiFi interface. Please check your network connections.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Using WiFi interface: {interface}\")\n",
    "    \n",
    "    X, y = capture_packets(interface)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    y_list = y  # Keep y as a list of strings\n",
    "    \n",
    "\n",
    "     # Generate timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Save the dataset\n",
    "    torch.save((X_tensor, y_list), f'packet_dataset_{timestamp}_{CAPTURE_DURATION}.pt')\n",
    "    \n",
    "    print(f\"Dataset saved. Shape: {X_tensor.shape}, Labels: {len(y_list)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train new prioritizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import datetime\n",
    "from os.path import isfile\n",
    "from sys import argv\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.amp import autocast\n",
    "\n",
    "\n",
    "\n",
    "class PacketPrioritizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=10, hidden_size=64, num_layers=2, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 6)  # 6 output classes for 6 priority levels\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = self.relu(self.fc1(lstm_out[:, -1, :]))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def train_model(model, train_loader, device, num_epochs=50):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with autocast(\"cuda\"):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "def prepare_data(X, y):\n",
    "    # Convert priorities to class indices (0-5)\n",
    "    priority_to_class = {\n",
    "        'Games': 0,\n",
    "        'Real Time': 1,\n",
    "        'Streaming': 2,\n",
    "        'Normal': 3,\n",
    "        'Web download': 4,\n",
    "        'App download': 5\n",
    "    }\n",
    "    y_classes = torch.tensor([priority_to_class[p] for p in y])\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(X, y_classes)\n",
    "    return DataLoader(dataset, batch_size=128, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "\n",
    "def main(datasetName):\n",
    "    \n",
    "    if datasetName == None:\n",
    "        while True:\n",
    "            datasetName = input(\"Enter the dataset (.pt) file name:\")\n",
    "            if isfile(datasetName) == False:\n",
    "                print(f\"File doesnt exist.{datasetName} Retry.\")\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Check if CUDA is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    X, y = torch.load(datasetName, weights_only=True)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_loader = prepare_data(X, y)\n",
    "    \n",
    "    # Initialize the model and move it to GPU\n",
    "    model = PacketPrioritizer().to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, device)\n",
    "    \n",
    "      \n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # updated PTH model's filename\n",
    "    new_PTH_FILENAME = f\"packet_prioritizer_{timestamp}.pth\"\n",
    "\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), new_PTH_FILENAME)\n",
    "    \n",
    "    print(\"Model trained and saved successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file = argv[1] if len(argv) > 1 else None\n",
    "    main(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrain existing prioritixer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from os.path import isfile\n",
    "import datetime\n",
    "from sys import argv\n",
    "\n",
    "# globals\n",
    "PTH_FILENAME = None\n",
    "newDatasetName = None\n",
    "\n",
    "class PacketPrioritizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=10, hidden_size=64, num_layers=2, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, 6)  # 6 output classes for 6 priority levels\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = self.relu(self.fc1(lstm_out[:, -1, :]))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def train_model(model, train_loader, device, num_epochs=50):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "def prepare_data(X, y):\n",
    "    priority_to_class = {\n",
    "        'Games': 0,\n",
    "        'Real Time': 1,\n",
    "        'Streaming': 2,\n",
    "        'Normal': 3,\n",
    "        'Web download': 4,\n",
    "        'App download': 5\n",
    "    }\n",
    "    y_classes = torch.tensor([priority_to_class[p] for p in y])\n",
    "    \n",
    "    dataset = TensorDataset(X, y_classes)\n",
    "    return DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "def main(pth_file, datasetName):\n",
    "    global PTH_FILENAME, newDatasetName\n",
    "    if pth_file == None and datasetName == None:\n",
    "        while True:\n",
    "            if pth_file == None:\n",
    "                PTH_FILENAME = input(\"Enter the previously trained model (.pth) file name:\")\n",
    "            if datasetName == None:\n",
    "                newDatasetName = input(\"Enter the training dataset (.pt) file name:\")\n",
    "            if isfile(PTH_FILENAME) == True and isfile(newDatasetName) == True:\n",
    "                break\n",
    "            else:\n",
    "                print(\"File(s) doesnt exist. Retry.\")\n",
    "    \n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    model = PacketPrioritizer()\n",
    "    model.load_state_dict(torch.load(PTH_FILENAME))\n",
    "    model.to(device)\n",
    "    print(\"Pre-trained model loaded successfully.\")\n",
    "\n",
    "    # Load the new dataset\n",
    "    X, y = torch.load(newDatasetName)\n",
    "    \n",
    "    # Prepare data\n",
    "    train_loader = prepare_data(X, y)\n",
    "    \n",
    "    # Train the model again\n",
    "    print(\"Starting additional training...\")\n",
    "    train_model(model, train_loader, device)\n",
    "    \n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # updated PTH model's filename\n",
    "    new_PTH_FILENAME = f\"packet_prioritizer_{timestamp}.pth\"\n",
    "\n",
    "    # Save the updated model\n",
    "    torch.save(model.state_dict(), new_PTH_FILENAME)\n",
    "    \n",
    "    print(f\"Model retrained and saved successfully as {new_PTH_FILENAME}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file1,file2 = None, None if len(argv) < 2 else argv[1], argv[2]\n",
    "    main(file1, file2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
